{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f33729-bc0d-43b2-8f71-30748e70d14c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Pitti\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import signal\n",
    "import math\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Path to the directory containing your CSV files\n",
    "  # Update this path to your specific directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af793ae7-8a60-4dd1-8f6c-05696d12ddde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datacaller(filename, Comp_Freq, timewindow, stepsize, samplerate, **kwargs):\n",
    "    \n",
    "    def component_maker(df, timewindow, stepsize, samplerate, **kwargs):\n",
    "        bl_length = kwargs.get('bl_length', 0)\n",
    "        \n",
    "        baseline = df.iloc[:, 0:int((bl_length/1000)*samplerate)].mean(axis=1)\n",
    "        \n",
    "        startp = int((bl_length/1000)*samplerate)+1\n",
    "        endvar = 0\n",
    "        ec = 1\n",
    "        while startp < len(df.iloc[0, :]) and endvar == 0:\n",
    "            endp = int(startp+(timewindow/1000)*samplerate)\n",
    "            if endp >= len(df.iloc[0, :]):\n",
    "                endp = len(df.iloc[0, :])\n",
    "                endvar = 1\n",
    "            new_col_name = f'Epoch_{ec}'\n",
    "            ec += 1\n",
    "            if startp == int((bl_length/1000)*samplerate)+1 or startp == 0:\n",
    "                res = pd.DataFrame(df.iloc[:, startp:endp].mean(axis=1)-baseline, columns=[new_col_name])\n",
    "            else:\n",
    "                res[new_col_name] = df.iloc[:, startp:endp].mean(axis=1)-baseline\n",
    "            startp = int(startp+(stepsize/1000)*samplerate)\n",
    "        j=1\n",
    "        for i in features:\n",
    "            res[f\"F_{j}\"] = i\n",
    "            j +=1\n",
    "        return res\n",
    "\n",
    "    def frequency_maker(df, timewindow, stepsize, samplerate, **kwargs):\n",
    "        baseline = kwargs.get('baseline', 0)\n",
    "        freq = kwargs.get('freq', 0)\n",
    "        \n",
    "        eeg_data = df.iloc[:, 0:int((baseline/1000)*samplerate)]\n",
    "        lim = len(df.iloc[0, :])\n",
    "        frequencies, psd_matrix = signal.welch(eeg_data, fs=samplerate, nperseg=int((baseline/1000)*samplerate))\n",
    "        basepower = []\n",
    "        for i in range(len(eeg_data)):\n",
    "            basepower.append(psd_matrix[i][(np.abs(frequencies - freq)).argmin()])\n",
    "        basepower = np.array(basepower)\n",
    "\n",
    "        startp = int((baseline/1000)*samplerate)+1\n",
    "        endc = 0\n",
    "        ec = 1\n",
    "        while startp < lim and endc == 0:\n",
    "            endp = int(startp+(timewindow/1000)*samplerate)\n",
    "            if endp >= lim:\n",
    "                endp = lim\n",
    "                endc = 1\n",
    "\n",
    "            eeg_data = df.iloc[:, startp:endp]\n",
    "            frequencies, psd_matrix = signal.welch(eeg_data, fs=samplerate, nperseg=(endp-startp))\n",
    "            powers = []\n",
    "            new_col_name = f'Epoch_{ec}'\n",
    "            ec += 1\n",
    "            for i in range(len(eeg_data)):\n",
    "                powers.append(10*(math.log10(psd_matrix[i][(np.abs(frequencies - freq)).argmin()]/basepower[i])))\n",
    "            if startp == int((baseline/1000)*samplerate)+1:\n",
    "                res = pd.DataFrame(powers, columns=[new_col_name])\n",
    "            else:\n",
    "                res[new_col_name] = powers\n",
    "            startp = int(startp+(stepsize/1000)*samplerate)\n",
    "        j=1\n",
    "        for i in features:\n",
    "            res[f\"F_{j}\"] = i\n",
    "            j +=1\n",
    "        return res\n",
    "    \n",
    "    df = pd.read_csv(filename).T\n",
    "    features = filename.replace(directory,'').replace('.csv','').replace('/','').split(\"_\")\n",
    "    additional_args = {}\n",
    "    for key, value in kwargs.items():\n",
    "        additional_args[key] = value\n",
    "    \n",
    "    if Comp_Freq == \"Comp\":\n",
    "        return component_maker(df, timewindow, stepsize, samplerate, **additional_args)\n",
    "    elif Comp_Freq == \"Freq\":\n",
    "        return frequency_maker(df, timewindow, stepsize, samplerate, **additional_args)\n",
    "    else:\n",
    "        print(\"ISSUE\")\n",
    "        \n",
    "def parallel_datacaller(args):\n",
    "    # Unpack the arguments\n",
    "    filename, Comp_Freq, timewindow, stepsize, samplerate, kwargs = args\n",
    "    # Call the datacaller function with the specified arguments\n",
    "    return datacaller(filename, Comp_Freq, timewindow, stepsize, samplerate, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f6c220-3e27-42cd-be48-871f56ecc21c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12400\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('F_1', 'F_3', 'F_5', 'F_6')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:171\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:214\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:222\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:114\u001b[0m, in \u001b[0;36mpandas._libs.index._unpack_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ('F_1', 'F_3', 'F_5', 'F_6')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Create a Pool of workers\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#with Pool(processes=4) as pool:  # Adjust the number of processes as needed\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Execute datacaller function in parallel\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Concatenate the resulting DataFrames into one big DataFrame\u001b[39;00m\n\u001b[0;32m     24\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(res, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m X_categorical \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF_3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF_5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF_6\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m X_continuous \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Combine categorical and continuous variables\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: ('F_1', 'F_3', 'F_5', 'F_6')"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "if __name__ == '__main__':\n",
    "    directory = \"C:/Users/Pitti/Desktop/Study2_AI/Data/RawEEG_ERP/\"\n",
    "\n",
    "    # Get the list of files in the directory\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.csv') and '_S_' in f]\n",
    "\n",
    "\n",
    "    # Define the arguments for datacaller function\n",
    "    args_list = [(f, \"Comp\", 100, 50, 500, {'bl_length': 500}) for f in files]\n",
    "    print(len(args_list))\n",
    "    \n",
    "    res = []\n",
    "    for i in args_list:\n",
    "        res.append(parallel_datacaller(i))\n",
    "\n",
    "    # Create a Pool of workers\n",
    "    #with Pool(processes=4) as pool:  # Adjust the number of processes as needed\n",
    "        # Execute datacaller function in parallel\n",
    "        #results = list(tqdm(pool.imap(parallel_datacaller, args_list), total=len(args_list)))\n",
    "        #results = pool.map(parallel_datacaller, args_list)\n",
    "\n",
    "    # Concatenate the resulting DataFrames into one big DataFrame\n",
    "    df = pd.concat(res, axis=0)\n",
    "\n",
    "    X_categorical = pd.get_dummies(df['F_1', 'F_3', 'F_5', 'F_6'])\n",
    "    X_continuous = [col for col in df.columns if 'Ep' in col]\n",
    "\n",
    "    # Combine categorical and continuous variables\n",
    "    X = pd.concat([X_categorical, X_continuous], axis=1)\n",
    "\n",
    "    # Assuming 'Mindset' is the target variable for classification\n",
    "    y = df['F_4']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Initialize and train a RandomForestClassifier model\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    feature_selector = SelectFromModel(rf_model, threshold='median')\n",
    "\n",
    "    # Fit the feature selector to the training data\n",
    "    X_train_selected = feature_selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = feature_selector.transform(X_test)\n",
    "    # Get the selected features\n",
    "    selected_feature_indices = feature_selector.get_support(indices=True)\n",
    "    selected_features = X_train.columns[selected_feature_indices]\n",
    "\n",
    "    # Print the selected features\n",
    "    print(\"Selected Features:\", selected_features)\n",
    "    # Define the hyperparameters grid\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    rf_model.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = rf_model.predict(X_test_selected)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Initialize the RandomForestClassifier model\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    # Perform Grid Search to find the best hyperparameters\n",
    "    grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Get the best parameters and the best score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best Score (Accuracy):\", best_score)\n",
    "\n",
    "    # Train the model with the best hyperparameters\n",
    "    best_rf_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "    best_rf_model.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = best_rf_model.predict(X_test_selected)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test Set Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cd39454-dcdb-41b0-be9b-9465761aa9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8373118279569892\n",
      "Selected Features: Index(['F_1_NTh', 'F_1_Th', 'F_3_05', 'F_3_06', 'F_3_07', 'F_3_08', 'F_3_09',\n",
      "       'F_3_19', 'F_3_20', 'F_3_22', 'F_3_23', 'F_3_24', 'F_3_25', 'F_3_26',\n",
      "       'F_3_27', 'F_3_32', 'F_3_33', 'F_3_35', 'F_3_38', 'F_5_post', 'F_5_pre',\n",
      "       'Epoch_1', 'Epoch_2', 'Epoch_3', 'Epoch_4', 'Epoch_5', 'Epoch_6',\n",
      "       'Epoch_7', 'Epoch_8', 'Epoch_9', 'Epoch_10', 'Epoch_11', 'Epoch_12',\n",
      "       'Epoch_13', 'Epoch_14', 'Epoch_15', 'Epoch_16', 'Epoch_17', 'Epoch_18',\n",
      "       'Epoch_19', 'Epoch_20', 'Epoch_21', 'Epoch_22', 'Epoch_23', 'Epoch_24',\n",
      "       'Epoch_25', 'Epoch_26', 'Epoch_27', 'Epoch_28', 'Epoch_29', 'Epoch_30',\n",
      "       'Epoch_31', 'Epoch_32', 'Epoch_33', 'Epoch_34', 'Epoch_35', 'Epoch_36',\n",
      "       'Epoch_37', 'Epoch_38', 'Epoch_39', 'Epoch_40', 'Epoch_41', 'Epoch_42',\n",
      "       'Epoch_43', 'Epoch_44', 'Epoch_45', 'Epoch_46', 'Epoch_47', 'Epoch_48',\n",
      "       'Epoch_49'],\n",
      "      dtype='object')\n",
      "Accuracy: 0.8306451612903226\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'param_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Initialize GridSearchCV\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mrf_model, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Perform Grid Search to find the best hyperparameters\u001b[39;00m\n\u001b[0;32m     55\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'param_grid' is not defined"
     ]
    }
   ],
   "source": [
    "X_categorical = pd.get_dummies(df[['F_1', 'F_3', 'F_5', 'F_6']])\n",
    "X_continuous = df[[col for col in df.columns if 'Ep' in col]]\n",
    "\n",
    "# Combine categorical and continuous variables\n",
    "X = pd.concat([X_categorical, X_continuous], axis=1)\n",
    "\n",
    "# Assuming 'Mindset' is the target variable for classification\n",
    "y = df['F_4']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize and train a RandomForestClassifier model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "\n",
    "feature_selector = SelectFromModel(rf_model, threshold='median')\n",
    "\n",
    "# Fit the feature selector to the training data\n",
    "X_train_selected = feature_selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = feature_selector.transform(X_test)\n",
    "# Get the selected features\n",
    "selected_feature_indices = feature_selector.get_support(indices=True)\n",
    "selected_features = X_train.columns[selected_feature_indices]\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected Features:\", selected_features)\n",
    "# Define the hyperparameters grid\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "# recode the Subject number and the features colnames\n",
    "# apply model to O group\n",
    "# Try it with frequencies\n",
    "# Develop big model, add it to Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb802b98-38d4-4e53-ba7c-0977bea9d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the RandomForestClassifier model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Perform Grid Search to find the best hyperparameters\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score (Accuracy):\", best_score)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_rf_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_rf_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_rf_model.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Set Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
